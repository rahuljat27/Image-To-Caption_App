# Image Captioning App

This project is a web application that generates descriptive captions for uploaded images. It's built with Streamlit and uses a custom-built Vision Encoder-Decoder model to understand the content of an image and generate a coherent, natural-language caption.

The model was trained on the Flickr8k dataset, a widely-used benchmark for image captioning tasks.

---

### Live Demo

You can try the live application here: <https://image-to-caption-generator-tool.streamlit.app/>

---

### Technical Details

#### Model Architecture
The core of this project is a **Vision Encoder-Decoder** model implemented from scratch using PyTorch. The model's architecture is inspired by transformer-based models and is composed of two main components:

* **Vision Encoder**: This module processes the input image. It works by first breaking the image into smaller, non-overlapping patches. It then uses a series of self-attention blocks to learn the spatial relationships and features of these patches, creating a rich contextual embedding of the image.

* **Decoder**: This is a text-generating module. It uses a series of transformer decoder blocks with both self-attention and cross-attention. The cross-attention mechanism allows it to "look at" the image features generated by the encoder while predicting the next word in the caption sequence. This is a crucial step that aligns the visual data with the linguistic output.

#### Training and Inference
The model was trained on the Flickr8k dataset to learn the mapping from images to captions. For inference, the application uses a **greedy decoding strategy** with a temperature parameter to generate the caption word by word, starting with a special start-of-sequence (`[CLS]`) token and ending when an end-of-sequence (`[SEP]`) token is generated.

---

### Key Features

* **Custom Vision Model**: Uses a self-contained Vision Encoder-Decoder model built from scratch, demonstrating a deep understanding of the architecture.
* **Real-time Captioning**: Generates a caption for any user-uploaded image in real-time.
* **Intuitive UI**: A simple and clean user interface, powered by Streamlit, that makes the application easy for anyone to use.

---

### Technologies Used

* **Streamlit**: For building the interactive web application.
* **PyTorch**: The deep learning framework used to define and train the model.
* **Hugging Face `transformers`**: Utilized for the `distilbert-base-uncased` tokenizer to handle text processing.
* **Pillow (PIL)** and **`torchvision`**: For handling and preprocessing image data.

---

### How to Run Locally

To get a local copy of the project running, follow these simple steps.

#### Prerequisites

Make sure you have **Python 3.8** or newer installed on your system.

#### Installation

1.  **Clone the repository**:

    ```bash
    git clone [https://github.com/rahuljat27/Image-To-Caption_App.git](https://github.com/rahuljat27/Image-To-Caption_App.git)
    cd Image-To-Caption_App
    ```

2.  **Create a virtual environment** (highly recommended to avoid dependency conflicts):

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install the required packages**:

    ```bash
    pip install -r requirements.txt
    ```

4.  Ensure your trained model file (`caption_model_epoch29.pth`) is in the project directory.

#### Running the App

Once everything is set up, run the Streamlit application from your terminal:

```bash
streamlit run app.py
```

This will launch the app in your default web browser.

---

### Deployment

This application is designed to be easily deployed on **Streamlit Community Cloud**. The necessary `app.py`, `requirements.txt`, and model files (`caption_model_epoch29.pth`) are all present in the repository, enabling a one-click deployment.
